一：crawl_main.py文件的逻辑是负责开启通道
二：crawl_task.py文件是负责主要的逻辑
三:关于使用：
crawl_main.py为程序入口，直接执行即可。
如果设备第一次运行该代码请先安装依赖模块，但是全部安装完成以后再次启动时可能还会失败，是因为fake_useragent 随机生成UA的模块
在第一次使用时需要远程连接该模块服务，但是经常会处于超时所以请先检测该模块是否已经请求过服务。

四：注意事项：
因为该任务系统是多通道执行，而且是异步通信，一个任务爬取过的url都会记录在内存中，有的网站内包含的url量巨大，所以开启的通道数
不要太多，而且爬取深度不宜设置的过深，这样可能造成内存的溢出。所以请根据设备资源灵活调整。

五：不完美的地方
1：当时立项之时需求不明确，是基于sqlserver开发，后续存储数据的量巨大，存储数据改用mongodb，所以现在使用了两种数据库。
存储编辑下发任务的数据库在sqlserver上，而存储数据则在mongodb，由于后续任务的繁重和其他因素如果本人没有完善，请将任务
存储任务的数据表移到mongodb，因为可视化界面展示任务详情依赖sqlserver，所以请注意更改相应的可视化界面的后台查询语句和展示语句。


